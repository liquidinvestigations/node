[cluster]

nomad_url = http://10.66.60.1:4646
consul_url = http://10.66.60.1:8500
vault_url = http://10.66.60.1:8200

# Path to ini file with vault secrets.
vault_secrets = ../cluster/var/vault-secrets.ini

# Set this path to the cluster source directory, but only if you are running
# cluster v0.13+ installed directly on the host machine (with no `cluster` docker
# container).
;cluster_path = ../cluster/


[liquid]

# Domain for the liquid bundle.
domain = liquid.example.org

# User-friendly title.
# Defaults to the domain with no spaces, capitalized.
title = Liquid Example Org

# Always serving http. With https enabled, this redirects.
http_port = 80

# Set all applications in debug mode.
# Please
#        do
#           not
#               use
#                   in
#                      production.
debug = true

# Mount liquidinvestigations and hoover repositories.
mount_local_repos = false

# Path to a directory that contains clones of search and snoop.
# Defaults to ./repos/hoover
; hoover_repos_path = /path/to/hoover_org

# Path to a directory that contains a clone of liquidinvestigations/core.
# Defaults to ./repos/liquidinvestigations
; liquidinvestigations_repos_path = /path/to/liquidinvestigations_org

# Directory where docker volumes are mounted.
# This flag is used for import/export operations and checking if a collection
# has been initialized or not, so take care in setting it to the value of the
# Nomad Client meta flag "liquid_volumes".
; volumes = /path/to/volumes

# Defaults to ./collections
; collections = /path/to/collections

# Allow only staff to log in (e.g. during maintenance)
auth_staff_only = false

# Automatically kill all login sessions after this time.
# Does not work for RocketChat, configure that with the "rocketchat_autologout_days" setting below.
# Defaults to 100 days.
;auth_auto_logout = 12h

# Two-factor authentication
two_factor_auth = false

# Configure memory limits for Elasticsearch
# See https://www.elastic.co/guide/en/elasticsearch/reference/6.2/heap-size.html
# Values are in MB. Defaults are:
;elasticsearch_heap_size = 1024
;elasticsearch_memory_limit = 1536

# Add elasticsearch data nodes. They will have the same resources as the master node.
# This number can be freely increased to use more heap memory, in multiples of
# about 30GB.  To decrease, run command `'./liquid remove-last-es-data-node` and
# then decrease the value by 1 and re-deploy.
# Default is 0. Set with:
;elasticsearch_data_node_count = 1

# Configure memory limits for Tika
# The value is in MB. The default is:
;tika_memory_limit = 2500
# Configure replication factor for tika.
# The default is 1.
;tika_count = 3

# Configure memory limits for other apps. Use this when you encounter
# "OOM Killed" messages for these services. Start with these doubled
# limits:
;hypothesis_memory_limit = 2048
;nextcloud_memory_limit = 1024

# Rate limits for Hoover API
# The value is "x,y" (x requests every y seconds)
# Minimum recommended value is 12 requests / user / second, i.e. 360,30.
# Increase if you see `rate limit reached` errors in normal use.
;hoover_ratelimit_user = 200,10

# Configure memory limit (default: 300MB) and
# container count (default: 1, max: 3) for Hoover's web servers.
# These apply to both search and snoop; so actual usage will be doubled.
# Use `./liquid resources` to check the memory usage of your configuration.
# Increase in case of OOMs:
;hoover_web_memory_limit = 1500
;hoover_web_count = 3

# Sets user-facing URLs to this protocol.
# If not set, will check for the presence of the `[https]` section below.
# It makes sense to set this to `https` when another service will handle HTTPS
# termination in front of Traefik.
;http_protocol_override = https

# Routes to this URL for the Hoover UI instead of to the one hosted by Liquid.
# Open a port on the host machine (through ssh -R or VPN) and set it here:
;hoover_ui_override_server = 10.66.60.1:8888

# Force-pull the UI docker container image
;hoover_ui_force_pull = true

# The aggregation queries for facets are split into this many requests.
# Higher numbers here mean slower, less resource intensive searches. Default is 1.
;hoover_ui_agg_split = 2

# Max number of retries to attempt in the UI when doing search queries.
# Higher numbers here means slower searches will be returned. Default is 1.
;hoover_ui_search_retry = 2

# Maximum number of elasticsearch shards that the search page will hit at the
# same time for a given request. Each collection is stored in 5 shards per
# default elasticsearch settings. Default value is 3, you can override it here.
# Turn this value up to reduce search latency at the cost of resource impact.
;hoover_es_max_concurrent_shard_requests = 5

# Enable Hoover Maps.
# Requires a manual download of 130 GB for tiles and map index for entire planet.
# Please follow instructions in docs/Maps.md before enabling this flag.
; hoover_maps_enabled = true

# Removes the Rocket.Chat login form, only leaving the button. Set this after
# the OAuth manual step is done.
;rocketchat_show_login_form = false

# Uncomment this line to enable Rocketchat Push notifications. Default is `false`.
# This uses the public RocketChat Gateway and their integrations with Apple and Google.
# After enabling the flag you need to create a free RocketChat Cloud account
# and add a Self-Hosted Registration Token to the Connection Services tab from
# RocketChat Admin.
;rocketchat_enable_push = true

# Set number of days after which the RocketChat client requires a re-login.
# RocketChat is not affected by the "auth_auto_logout" setting above.
# Default is 100. Recommended for mobile app usage: 5-30 days.
;rocketchat_autologout_days = 6

# Version set to track for running `./liquid-update`.
# Possible values:
# - production (default) - tracking fixed versions for every service
# - testing - tracking the unstable `master` branches for all services
# Any version can be locally overridden by setting it in versions.ini or
# liquid.ini (the "versions" section).
;version_track = testing


# Uncomment this section to have Traefik use Let's Encrypt and open
# a port for HTTPS. HTTP traffic is not affected.
;[https]

;acme_email = you@yourdomain.bg
;https_port = 443
# Choose from the Let's Encrypt staging or production servers
;acme_caServer = https://acme-v02.api.letsencrypt.org/directory
;acme_caServer = https://acme-staging-v02.api.letsencrypt.org/directory


;[apps]
# Apps can be enabled or disabled here. Uncomment this section to continue.

# Shortcut; The default value is 'on'. 'liquid' and 'ingress' are always
# enabled, even if this value is set to 'off'.
;default_app_status = off

# Enable / disable each app.
# All possible values are:
;hoover = on
;nextcloud = on
;dokuwiki = on
;rocketchat = on
;hypothesis = on
;codimd = on


[deploy]

# Health check interval and timeout for all services
# Increase check_interval to lower idle system load at the cost
# of higher deploy times.
check_interval = 26s
check_timeout = 23s

# Configure how to poll health when running `./liquid deploy`
wait_max_sec = 770
wait_interval = 3
wait_green_count = 6


[snoop]

# General settings for Hoover Snoop
# --------------------------------
# - `enable_workers` - set to "false" to stop all processing.
# - `min_workers_per_node` - Minimum number of workers on each node, default 2.
# - `max_workers_per_node` - Maximum number of workers on each node, default 25.
# - `worker_cpu_count_multiplier` - Each node tries to run a number of workers
#                                   equal to its CPU count multiplied by this
#                                   value. The resulting worker count is
#                                   trimmed to the (min, max) values above.
#                                   Defaults to 0.55.
# - `rabbitmq_memory_limit` - memory limit of the queue container, expressed in Mb.
#                             Default 700, increase for larger collections.
# - `postgres_memory_limit` - shared memory limit for postgresql container.
#                             Default 1600, increase for larger collections,
#                             larger collection counts, or more workers.
#                             The memory attributed to postgres must be at least:
#                             64Mb * postgres_max_connections + 256Mb.
# - `postgres_max_connections` - max connection count for postgresql container.
#                             All worker and server processes open one connection per thread.
#                             Default 250, increase for larger worker pools and collection counts.
# - `max_result_window` - Default value for elasticsearch index setting "max_result_window".
#                         Defaults to 10000.
#                         Any collection can override this value.
# - `refresh_interval`  - Default value for elasticsearch index setting "refresh_interval".
#                         Defaults to "1s". Is set to "-1" when batch indexing happens.
#                         Any collection can override this value.


# PDF Preview and Thumbnails
# --------------------------------
# - `pdf_preview_enabled`  - Set to "true" to run PDF preview service. This
#                            converts various document types into PDF for previewing them in the UI.
# - `pdf_preview_count`    - Set the number of containers to run, default 1.
# - `pdf_preview_memory_limit`    - Override memory limit for container, default 1500.
# - `thumbnail_generator_enabled` - Enable Thumbnail Generator Service. Default is 'false'
# - `thumbnail_generator_memory_limit` - Configure memory limits for Thumbnail Generator Service
#                                        The value is in MB. The default is: 1500.
# - `thumbanil_generator_count` - Set the number of containers to run. The default is 1.


# Image Classification and Object Detection
# --------------------------------
# - `image_classification_count`    - Set the number of containers to run, default 1.
#                                     Each container runs 4 processes, so scale up the count to 2-3 containers
#                                     to cover about 40% of the total snoop worker count.
# - `image_classification_memory_limit` - Override memory limit for container, default 2500.
# - `image_classification_object_detection_enabled` - Set the service to enable object detection. Default is "false".
# - `image_classification_object_detection_model`   - Set the model used for object detection. Default is "yolo".
#                                                     Options are "retinanet", "yolo" and "tiny_yolo"
# - `image_classification_classify_images_enabled` - Set the service to enable image classification. Default is "false".
# - `image_classification_classify_images_model`   - Set the model used for image classification. Default is "mobilenet".
#                                                    Options are "mobilenet", "resnet", "inception" and "densenet".
# For documentation of the image classification settings see:
# https://github.com/liquidinvestigations/image-classification-service


# Language Detection, Named Entity Recognition, Translation
# --------------------------------------------------------------
# - `nlp_language_detection_enabled`  - Setting to enable language detection. Default is "false".
# - `nlp_entity_extraction_enabled`   - Setting to enable entity extraction. Default is "false".
#                                       Requires language detection to be enabled.
# - `nlp_count`                       - Set the number of containers to run. Default is 1.
#                                       Each container runs 8 processes, so scale up
#                                       to cover about 50% of the total snoop worker count.
# - `nlp_fallback_language` - Language that language detection returns if no
#                             language can be determined by service.
# - `nlp_spacy_text_limit`  - Number of chars that spacy processes at once. Since
#                             Spacy allocates roughly 1 GB per 100000 chars, the
#                             service breaks if not enough memory is available.
#                             Default is 40000.
# - `nlp_memory_limit`      - Memory for nlp service, default is 5500 mb.
# - `translation_enabled`  - Enable translation using LibreTranslate. Default is "false".
#                            Requires language detection to be enabled.
# - `translation_count`    - Set the number of containers  to run. Default is 1.
#                            Each container runs 8 workers and saturates 8 CPU cores; scale up
#                            to cover around 80% of the total cluster CPU count.
# - `translation_memory_limit` - Set the memory limit for each container. Default is 2500 mb.
# - `translation_target_languages` - 2-letter language codes, divided by comma.
#                                    Any text found in another language will be
#                                    translated into these languages.
#                                    Default "en". Set to "en,de" to translate into German too.
# - `translation_text_length_limit` - Limit the amount of text sent to the translation engine.
#                                     Default is 400, about the length of a small paragraph.
#                                     Translating such a paragraph takes between 2-25 seconds, so
#                                     increasing this value is recommended only after the whole
#                                     collection has been completed. Note: after changing this value,
#                                     you must manually reset all related tasks.

enable_workers = true
min_workers_per_node = 1
max_workers_per_node = 12
worker_cpu_count_multiplier = 0.6
; rabbitmq_memory_limit = 700
; postgres_memory_limit = 2600
; postgres_max_connections = 400
; max_result_window = 10000
; refresh_interval = 1s

pdf_preview_enabled = true
thumbnail_generator_enabled = true
thumbanil_generator_count = 1
; thumbnail_generator_memory_limit = 900

image_classification_classify_images_enabled = true
; image_classification_count = 1
; image_classification_memory_limit = 1000

# WARNING: Enabling this takes a very long time to process, around 15-30s/picture.
image_classification_object_detection_enabled = false
; image_classification_object_detection_model = retina


; nlp_language_detection_enabled = true
; nlp_entity_extraction_enabled = true
; nlp_count = 1
; nlp_fallback_language = de
;
; translation_enabled = true
; translation_count = 2
; translation_target_languages = en,de
; translation_text_length_limit = 1000

[versions]
# Override versions of docker images from versions.ini:

# liquid-core = liquidinvestigations/core:latest

# Optional and Custom Jobs
# ========================

# Run a zipkin tracing system.
; [job:zipkin]
; template = templates/zipkin.nomad

# Run a custom local job, see authdemo.py
# [job:authdemo]
# loader = local_jobs.authdemo.Authdemo



# Hoover Collections
# ==================
# These settings are specific to the collection:
# ---------------------------------------------
# - collection names    - are [a-z][a-z0-9] only; no underscores or dashes allowed!
# - process             - Disable the queueing of tasks for this collection. Tasks already on the queue will still run.
# - sync                - Enable periodic re-walking of input and imported OCR data.
# - ocr_languages       - Comma-separated list of three-leter language codes for Tesseract 4.0:
#                         https://tesseract-ocr.github.io/tessdoc/Data-Files#data-files-for-version-400-november-29-2016

# The following collection settings all override the [snoop] setting of the same name:
# ---------------------------------------------
# - max_result_window
# - refresh_interval
# - pdf_preview_enabled
# - thumbnail_generator_enabled
# - image_classification_object_detection_enabled
# - image_classification_classify_images_enabled
# - nlp_language_detection_enabled
# - nlp_entity_extraction_enabled
# - nlp_fallback_language
# - translation_enabled
# - translation_target_languages
# - translation_text_length_limit

# Examples below.

# Collection corresponding to nextcloud uploads
; [collection:uploads]
; process = True
; sync = True
; ocr_languages = eng,ron


# Add the testdata collection. Remove if not needed.
# Be sure to clone it first:
#     git clone https://github.com/liquidinvestigations/testdata collections/testdata

; [collection:testdata]
; process = True
; ocr_languages = eng,ron
; pdf_preview_enabled = true
; thumbnail_generator_enabled = true
; image_classification_object_detection_enabled = true
; image_classification_classify_images_enabled = true
; nlp_language_detection_enabled = true
; nlp_entity_extraction_enabled = true
; nlp_fallback_language = en
; translation_enabled = true
; translation_target_languages = en,ru
; translation_text_length_limit = 200



# Self-Hosted Continuous Integration
# ==================================
;[ci]
;runner_capacity = 2
;docker_username = NOTHING
;docker_password = EVERYTHING
;github_client_id = SOMETHING
;github_client_secret = SOMETHING_ELSE
;github_user_filter = username,organization,anything
;target_hostname = krokodil.krokod.il
;target_port = 666
;target_username = ssh username
;target_password = ssh password
;target_test_admin_password = web password (username = admin)
