[cluster]

nomad_url = http://10.66.60.1:4646
consul_url = http://10.66.60.1:8500
vault_url = http://10.66.60.1:8200

# Path to ini file with vault secrets.
vault_secrets = ../cluster/var/vault-secrets.ini

# Set this path to the cluster source directory, but only if you are running
# cluster v0.13+ installed directly on the host machine (with no `cluster` docker
# container).
;cluster_path = ../cluster/

[liquid]

# Domain for the liquid bundle.
domain = liquid.example.org

# User-friendly title.
# Defaults to the domain with no spaces, capitalized.
title = Liquid Example Org

# Always serving http. With https enabled, this redirects.
http_port = 80

# Set all applications in debug mode.
# Please
#        do
#           not
#               use
#                   in
#                      production.
debug = true

# Mount liquidinvestigations and hoover repositories.
mount_local_repos = false

# Path to a directory that contains clones of search and snoop.
# Defaults to ./repos/hoover
; hoover_repos_path = /path/to/hoover_org

# Path to a directory that contains a clone of liquidinvestigations/core.
# Defaults to ./repos/liquidinvestigations
; liquidinvestigations_repos_path = /path/to/liquidinvestigations_org

# Directory where docker volumes are mounted.
# This flag is used for import/export operations and checking if a collection
# has been initialized or not, so take care in setting it to the value of the
# Nomad Client meta flag "liquid_volumes".
; volumes = /path/to/volumes

# Defaults to ./collections
; collections = /path/to/collections

# Allow only staff to log in (e.g. during maintenance)
auth_staff_only = false

# Automatically kill all login sessions after this time.
# Does not work for RocketChat, configure that with the "rocketchat_autologout_days" setting below.
# Defaults to 100 days.
;auth_auto_logout = 12h

# Two-factor authentication
two_factor_auth = false

# Configure memory limits for Elasticsearch
# See https://www.elastic.co/guide/en/elasticsearch/reference/6.2/heap-size.html
# Values are in MB. Defaults are:
;elasticsearch_heap_size = 1024
;elasticsearch_memory_limit = 1536

# Add elasticsearch data nodes. They will have the same resources as the master node.
# This number can be freely increased to use more heap memory, in multiples of
# about 30GB.  To decrease, run command `'./liquid remove-last-es-data-node` and
# then decrease the value by 1 and re-deploy.
# Default is 0. Set with:
;elasticsearch_data_node_count = 1

# Configure memory limits for Tika
# The value is in MB. The default is:
;tika_memory_limit = 2500
# Configure replication factor for tika.
# The default is 1.
;tika_count = 3

# Configure memory limits for other apps. Use this when you encounter
# "OOM Killed" messages for these services. Start with these doubled
# limits:
;nextcloud_memory_limit = 1024

# Rate limits for Hoover API
# The value is "x,y" (x requests every y seconds)
# Minimum recommended value is 12 requests / user / second, i.e. 360,30.
# Increase if you see `rate limit reached` errors in normal use.
;hoover_ratelimit_user = 200,10

# Configure memory limit (default: 300MB) and
# container count (default: 2, max: 3) for Hoover's web servers.
# These apply to both search and snoop; so actual usage will be doubled.
# Use `./liquid resources` to check the memory usage of your configuration.
# Increase in case of OOMs:
;hoover_web_memory_limit = 1500
;hoover_web_count = 3

# Sets user-facing URLs to this protocol.
# If not set, will check for the presence of the `[https]` section below.
# It makes sense to set this to `https` when another service will handle HTTPS
# termination in front of Traefik.
;http_protocol_override = https

# Routes to this URL for the Hoover UI instead of to the one hosted by Liquid.
# Open a port on the host machine (through ssh -R or VPN) and set it here:
;hoover_ui_override_server = 10.66.60.1:8888

# Force-pull the UI docker container image
;hoover_ui_force_pull = true

# The aggregation queries for facets are split into this many requests.
# Higher numbers here mean slower, less resource intensive searches. Default is 1.
;hoover_ui_agg_split = 2

# Max number of retries to attempt in the UI when doing search queries.
# Higher numbers here means slower searches will be returned. Default is 1.
;hoover_ui_search_retry = 2

# Maximum number of elasticsearch shards that the search page will hit at the
# same time for a given request. Each collection is stored in 5 shards per
# default elasticsearch settings. Default value is 3, you can override it here.
# Turn this value up to reduce search latency at the cost of resource impact.
;hoover_es_max_concurrent_shard_requests = 5

# Enable Hoover Maps.
# Requires a manual download of 130 GB for tiles and map index for entire planet.
# Please follow instructions in docs/Maps.md before enabling this flag.
; hoover_maps_enabled = true


# Enable Hoover Uploads.
; hoover_uploads_enabled = true

# Set delay for searches in the hoover search service to debug frontend bug
;hoover_search_debug_delay = 40

# Set parameter for async search in the UI
;hoover_ui_search_retry_delay = 3000
;hoover_ui_async_search_poll_interval = 45
;hoover_ui_async_search_max_final_retries = 3
;hoover_ui_async_search_error_multiplier = 2
;hoover_ui_async_search_error_summation = 60

# Removes the Rocket.Chat login form, only leaving the button. Set this after
# the OAuth manual step is done.
;rocketchat_show_login_form = false

# Uncomment this line to enable Rocketchat Push notifications. Default is `false`.
# This uses the public RocketChat Gateway and their integrations with Apple and Google.
# After enabling the flag you need to create a free RocketChat Cloud account
# and add a Self-Hosted Registration Token to the Connection Services tab from
# RocketChat Admin.
;rocketchat_enable_push = true

# Set number of days after which the RocketChat client requires a re-login.
# RocketChat is not affected by the "auth_auto_logout" setting above.
# Default is 100. Recommended for mobile app usage: 5-30 days.
;rocketchat_autologout_days = 6

# Version set to track for running `./liquid-update`.
# Possible values:
# - production (default) - tracking fixed versions for every service
# - testing - tracking the unstable `master` branches for all services
# Any version can be locally overridden by setting it in versions.ini or
# liquid.ini (the "versions" section).
;version_track = testing

# Uncomment to enable showing dashboards in home page for superuser admins.
# By default, dashboards are turned off.
;enable_superuser_dashboards = true

# Uncomment this section to have Traefik use Let's Encrypt and open
# a port for HTTPS. HTTP traffic is not affected.
;[https]

;acme_email = you@yourdomain.bg
;https_port = 443
# Choose from the Let's Encrypt staging or production servers
;acme_caServer = https://acme-v02.api.letsencrypt.org/directory
;acme_caServer = https://acme-staging-v02.api.letsencrypt.org/directory


[apps]
# Apps can be enabled or disabled here.

# Shortcut; The default value is 'on'. 'liquid' and 'ingress' are always
# enabled, even if this value is set to 'off'.
;default_app_status = off

# Enable / disable each app.
# All possible values are:
;hoover = on
;nextcloud = on
;dokuwiki = on
;rocketchat = on
;codimd = on
;wikijs = on


[deploy]

# Health check interval and timeout for all services
# Increase check_interval to lower idle system load at the cost
# of higher deploy times.
;check_interval = 26s
;check_timeout = 23s

# Configure how to poll health when running `./liquid deploy`
;wait_max_sec = 770
;wait_interval = 3
;wait_green_count = 6

# Scale up or down the CPU and MemoryMB requirements.
# This allows us to  deployment on small machines with every service,
# while also allowing for multi-host balancing on very large servers.
# Default value: 1. Valid values: [0.7-5.0].
;container_memory_limit_scale = 1.0
;container_cpu_scale = 1.0


[snoop]

# General settings for Hoover Snoop
# --------------------------------
# - `enable_workers`     - Set to "false" to stop all processing.
# - `container_process_count` - Sets number of processes inside each container.
#                               Valid values are from 1 to 8. Defaults to 1.

# `default_queue_worker_count`    - Set number of containers running tasks for the `default` queue.
#                                   Default 1. Each container runs `container_process_count` processes.
# `filesystem_queue_worker_count` - Set number of containers running tasks for the `filesystem` queue.
#                                   Default 1. Each container runs `container_process_count` processes.
# `ocr_queue_worker_count`        - Set number of containers running tasks for the `ocr` queue.
#                                   Default 1. Each container runs `container_process_count` processes.
# `digests_queue_worker_count`    - Set number of containers running tasks for the `digests` queue.
#                                   Default 1. Each container runs `container_process_count` processes.

# - `rabbitmq_memory_limit` - memory limit of the queue container, expressed in Mb.
#                             Default 700, increase for larger/more collections.
#                             For 3-5 large collections running concurrently, set this to 4-6 GB.
# - `postgres_memory_limit` - shared memory limit for postgresql container.
#                             Default 1600, increase for larger collections,
#                             larger collection counts, or more workers.
#                             The memory attributed to postgres must be at least:
#                             64Mb * postgres_max_connections + 256Mb.
# - `postgres_max_connections` - max connection count for postgresql container.
#                             All worker and server processes open one connection per thread.
#                             Default 250, increase for larger worker pools and collection counts.
# - `max_result_window` - Default value for elasticsearch index setting "max_result_window".
#                         Defaults to 10000.
#                         Any collection can override this value.
# - `refresh_interval`  - Default value for elasticsearch index setting "refresh_interval".
#                         Defaults to "1s". Is set to "-1" when batch indexing happens.
#                         Any collection can override this value.
# `skip_mime_types`       - Comma-separated list of mime types to exclude from processing.
# `skip_extensions`       - Comma-separated list of file extensions to exclude from processing.
#                           See examples below.
# `snoop_worker_omp_thread_limit` - Number of threads per process, for OCR and other OMP processes.
#                                   Default is 2.
# `snoop_ocr_parallel_pages`      - Number of parallel OCR processes per worker, for multi-page documents.
#                                   Default is 2.
#                                   Multiply this with `snoop_worker_omp_thread_limit` to obtain load per
#                                   worker.
# `snoop_unarchive_threads`      - Number of threads to use for unarchiving.
#                                   Default is 4, minimum 1 and maximum is 16.


# PDF Preview and Thumbnails
# --------------------------------
# - `pdf_preview_enabled`  - Set to "true" to run PDF preview service. This
#                            converts various document types into PDF for previewing them in the UI.
# - `pdf_preview_count`    - Set the number of containers to run, default 1.
# - `pdf_preview_memory_limit`    - Override memory limit for each process, default 600 MB/process.
# - `thumbnail_generator_enabled` - Enable Thumbnail Generator Service. Default is 'false'
# - `thumbnail_generator_memory_limit` - Configure memory limits for Thumbnail Generator Service
#                                        The value is in MB per process. The default is: 500 MB/process.
# - `thumbnail_generator_count` - Set the number of containers to run. The default is 1.


# Image Classification and Object Detection
# --------------------------------
# - `image_classification_count`    - Set the number of containers to run, default 1.
# - `image_classification_memory_limit` - Override memory limit for each container process, default 1100 MB/process.
# - `image_classification_object_detection_enabled` - Set the service to enable object detection. Default is "false".
# - `image_classification_object_detection_model`   - Set the model used for object detection. Default is "yolo".
#                                                     Options are "retinanet", "yolo" and "tiny_yolo"
# - `image_classification_classify_images_enabled` - Set the service to enable image classification. Default is "false".
# - `image_classification_classify_images_model`   - Set the model used for image classification. Default is "mobilenet".
#                                                    Options are "mobilenet", "resnet", "inception" and "densenet".
# For documentation of the image classification settings see:
# https://github.com/liquidinvestigations/image-classification-service


# Language Detection, Named Entity Recognition, Translation
# --------------------------------------------------------------
# - `nlp_language_detection_enabled`  - Setting to enable language detection. Default is "false".
# - `nlp_entity_extraction_enabled`   - Setting to enable entity extraction. Default is "false".
#                                       Requires language detection to be enabled.
# - `nlp_count`                       - Set the number of containers to run. Default is 1.
#                                       Each container runs 8 processes, so scale up
#                                       to cover about 50% of the total snoop worker count.
# - `nlp_fallback_language` - Language that language detection returns if no
#                             language can be determined by service.
# - `nlp_spacy_text_limit`  - Number of chars that spacy processes at once. Since
#                             Spacy allocates roughly 1 GB per 100000 chars, the
#                             service breaks if not enough memory is available.
#                             Default is 40000.
# - `nlp_memory_limit`      - Memory for each NLP service process, default is 900 MB / process.
# - `translation_enabled`  - Enable translation using LibreTranslate. Default is "false".
#                            Requires language detection to be enabled.
# - `translation_count`    - Set the number of containers  to run. Default is 1.
#                            Each container runs 8 workers and saturates 8 CPU cores; scale up
#                            to cover around 80% of the total cluster CPU count.
# - `translation_memory_limit` - Set the memory limit for each process. Default is 600 MB / process.
# - `translation_target_languages` - 2-letter language codes, divided by comma.
#                                    Any text found in another language will be
#                                    translated into these languages.
#                                    Default "en". Set to "en,de" to translate into German too.
# - `translation_text_length_limit` - Limit the amount of text sent to the translation engine.
#                                     Default is 400, about the length of a small paragraph.
#                                     Translating such a paragraph takes between 2-25 seconds, so
#                                     increasing this value is recommended only after the whole
#                                     collection has been completed. Note: after changing this value,
#                                     you must manually reset all related tasks.

enable_workers = true

# Number of processes in each container; value is global for all worker types.
# A good starting number for this is: (total number of cores in cluster / 12).
container_process_count = 1

# Number of containers for each queue
# Each container has `container_process_count` processes.
default_queue_worker_count = 1
filesystem_queue_worker_count = 1
ocr_queue_worker_count = 3
digests_queue_worker_count = 1

; rabbitmq_memory_limit = 700
; postgres_memory_limit = 2600
; postgres_max_connections = 400
; max_result_window = 10000
; refresh_interval = 1s

pdf_preview_enabled = true
pdf_preview_count = 1

thumbnail_generator_enabled = true
thumbnail_generator_count = 1

image_classification_classify_images_enabled = true
image_classification_count = 1
# WARNING: Enabling Object Detection takes a very long time to process, around 15-30s/picture.
; image_classification_object_detection_enabled = true

# nlp_language_detection_enabled = true
# nlp_entity_extraction_enabled = true
# nlp_count = 2
# nlp_fallback_language = en

# translation_enabled = true
# translation_count = 2
# translation_target_languages = en
# translation_text_length_limit = 1000

# Snoop Document Processing Skip
# ------------------------------
# Exclude files from processing by mime type or guessed extension.
# By default, we skip Windows and Linux installation files, executables, DLLs, and web source code (css/js).
# NOTE: - The documents are still ingested, hashed and indexed, but all other processes are skipped.
# Both lists are comma-separated, following this format:
; skip_mime_types=application/octet-stream,application/x-empty
; skip_extensions=.exe,.dll

# Default Values:
; skip_mime_types=application/octet-stream,application/x-dosexec,text/xml,application/x-setupscript,text/x-c,font/sfnt,image/x-win-bitmap,application/x-pnf,image/vnd.microsoft.icon,application/x-wine-extension-ini,application/x-empty,application/x-object,application/x-sharedlib,text/x-bytecode.python,text/x-shellscript,text/x-script.python,image/svg+xml,amd64.md5sums,amd64.list,amd64.triggers,amd64.shlibs,amd64.symbols,application/x-pie-executable,application/vnd.debian.binary-package,text/x-asm
; skip_extensions=.3gr,.acm,.adml,.admx,.aspx,.aux,.ax,.bin,.cat,.cdf-ms,.cdxml,.com,.config,.cpl,.css,.cur,.dat,.dll,.dll,.drv,.etl,.exe,.fon,.fot,.ico,.ime,.inf,.inf_loc,.ini,.js,.lnk,.man,.manifest,.mfl,.mof,.msc,.msi,.mui,.mum,.mun,.nls,.ocx,.pnf,.pnf,.pri,.ps1,.ps1xml,.psd1,.psm1,.resx,.scr,.sys,.tlb,.tte,.ttf,.vbx,.winmd,.xbf,.xml,.xrm-ms,.xsd

# OCR & other OpenMP parallelism (default 2)
;snoop_worker_omp_thread_limit = 2

# OCR page parallelism (default 2)
;snoop_ocr_parallel_pages = 2


[versions]
# Override versions of docker images from versions.ini:

# liquid-core = liquidinvestigations/core:latest

# Optional and Custom Jobs
# ========================

# Run a custom local job, write a configuration file (python class) and templates.
# A sample config file is kept under `examples/local_jobs/authdemo`.
# [job:authdemo]
# loader = examples.local_jobs.authdemo.Authdemo

# To enable a second Nextcloud instance, uncomment:
# [job:nextcloud_instance_2]
# loader = examples.local_jobs.nextcloud_instance_2.Nextcloud



# Hoover Collections
# ==================
# These settings are specific to the collection:
# ---------------------------------------------
# - collection names     - are [a-z][a-z0-9] only; no underscores or dashes allowed!
# - process              - Disable the queueing of tasks for this collection. Tasks already on the queue will still run.
# - sync                 - Enable periodic re-walking of input and imported OCR data.
# - ocr_languages        - Comma-separated list of three-leter language codes for Tesseract 4.0:
#                          https://tesseract-ocr.github.io/tessdoc/Data-Files#data-files-for-version-400-november-29-2016
# - explode_table_rows   - Create text records for each row in each table. Takes around 5min / 2000 rows;
#                          do not use for very large tables. Default: False.
# - default_table_header - Default column names for files without headers. Requires different headers separated by `;`.
#                          Columns inside headers are separated by `:`. Each header must have a unique column count.
#                          The following example sets default headers for 2-column tables and 4-column tables.
#                          Example: " Name:Age ; ID_Number:Name:Age:Sex ".
# - s3_blobs_address     - Optionally point collection to different Minio/S3 instance for Blobs storage.
# - s3_blobs_access_key  - S3 access key for blobs storage, required if previous value set.
# - s3_blobs_secret_key  - S3 secret key for blobs storage, required if previous value set.

# The following collection settings all override the [snoop] setting of the same name:
# ---------------------------------------------
# - max_result_window
# - refresh_interval
# - pdf_preview_enabled
# - thumbnail_generator_enabled
# - image_classification_object_detection_enabled
# - image_classification_classify_images_enabled
# - nlp_language_detection_enabled
# - nlp_entity_extraction_enabled
# - nlp_fallback_language
# - translation_enabled
# - translation_target_languages
# - translation_text_length_limit

# Examples below.

# Collection corresponding to nextcloud uploads
; [collection:uploads]
; process = True
; sync = True
; ocr_languages = eng,ron


# Add the testdata collection. Remove if not needed.
# Be sure to clone it first:
#     git clone https://github.com/liquidinvestigations/testdata collections/testdata

; [collection:testdata]
; process = True
; ocr_languages = eng,ron
; pdf_preview_enabled = true
; thumbnail_generator_enabled = true
; image_classification_object_detection_enabled = true
; image_classification_classify_images_enabled = true
; nlp_language_detection_enabled = true
; nlp_entity_extraction_enabled = true
; nlp_fallback_language = en
; translation_enabled = true
; translation_target_languages = en,ru
; translation_text_length_limit = 200


# Sentry Monitoring
# ===================
# Uncomment the section and set values with the Sentry DSN from its UI.
;[sentry]
;hoover-snoop = http://aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa@10.66.60.1:9000/2
;hoover-search = http://bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb@10.66.60.1:9000/3
;liquid-core = http://cccccccccccccccccccccccccccccccc@10.66.60.1:9000/4
;hoover-ui-server = http://dddddddddddddddddddddddddddddddd@10.66.60.1:9000/5
;hoover-ui-client = http://dddddddddddddddddddddddddddddddd@sentry.liquid.example.org/5
;proxy-to-subdomain = 10.66.60.1:9000


# Self-Hosted Continuous Integration
# ==================================
;[ci]
;runner_capacity = 2
;docker_username = NOTHING
;docker_password = EVERYTHING
;github_client_id = SOMETHING
;github_client_secret = SOMETHING_ELSE
;github_user_filter = username,organization,anything
;target_hostname = krokodil.krokod.il
;target_port = 666
;target_username = ssh username
;target_password = ssh password
;target_test_admin_password = web password (username = admin)
